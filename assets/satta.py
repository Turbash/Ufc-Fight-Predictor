# -*- coding: utf-8 -*-
"""Satta.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iTxPrs7JtLdChg0u2HLXRNode5sENLfE
"""

# Step 1.1: Import Libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1.2: Load Dataset
df = pd.read_csv("ufc-master.csv")

# Step 1.3: View Shape and Sample Rows
print(f"Dataset contains {df.shape[0]} rows and {df.shape[1]} columns.")
df.head()

# Step 1.4: List All Columns
columns = df.columns.tolist()
print("Total columns:", len(columns))
columns

# Step 2.1: Check missing values

# Sum of missing values per column
missing_values = df.isnull().sum()

# Filter to show only columns where missing values exist
missing_values = missing_values[missing_values > 0]

# Sort descending to show worst columns first
missing_values = missing_values.sort_values(ascending=False)

print(f"Total columns with missing data: {missing_values.shape[0]}")
missing_values

# Step 2.2: Visualize missing values

plt.figure(figsize=(12,6))
sns.barplot(x=missing_values.index, y=missing_values.values, palette="magma")
plt.xticks(rotation=90)
plt.ylabel("Number of Missing Values")
plt.title("Missing Values per Column")
plt.show()

# Step 2.3: Calculate percentage of missing values

missing_percentage = (missing_values / len(df)) * 100

missing_summary = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage (%)': missing_percentage
})

missing_summary

# Step 3.1: Review top columns with most missing data
missing_summary.sort_values(by="Percentage (%)", ascending=False).head(30)

# Step 3.3.1 Updated: Fill division rank missing values with 0 (means "Not ranked")
rank_columns = [col for col in df.columns if 'weight' in col.lower() and 'rank' in col.lower()]

for col in rank_columns:
    df[col] = df[col].fillna(0)

print(f"Filled {len(rank_columns)} weight class rank columns with 0.")

df[['BlueOdds', 'RedOdds']].isna().sum()

df.shape

df = df.dropna(subset=['RedOdds', 'BlueOdds'])

df.head(30)

# Fill other numerical columns with median (except rank columns)
numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
numerical_cols = [col for col in numerical_cols if col not in rank_columns]

for col in numerical_cols:
    if df[col].isnull().sum() > 0:
        df[col] = df[col].fillna(df[col].median())

print("Filled missing values in remaining numerical columns.")

df

df.columns

df.drop(columns=['EmptyArena'], inplace=True)

# Step 2.1: Check missing values

# Sum of missing values per column
missing_values = df.isnull().sum()

# Filter to show only columns where missing values exist
missing_values = missing_values[missing_values > 0]

# Sort descending to show worst columns first
missing_values = missing_values.sort_values(ascending=False)

print(f"Total columns with missing data: {missing_values.shape[0]}")
missing_values

df['BlueStance'].isna().sum()

df['BlueStance'] = df['BlueStance'].fillna('Orthodox')

df.shape

df[['FinishDetails', 'Finish', 'FinishRoundTime']].dtypes
df['FinishRoundTime'] = df['FinishRoundTime'].fillna('0:00')
df['Finish'] = df['Finish'].fillna('Unknown')
df['FinishDetails'] = df['FinishDetails'].fillna('Unknown')

sns.boxplot(data=df['BlueOdds'])
plt.show()

sns.boxplot(data=df['RedOdds'],color='red')
plt.show()

print(df.isnull().sum().sum())

# Step 3.6.1: Drop non-informative columns

columns_to_drop = [
    'RedFighter', 'BlueFighter',    # Fighter Names
    'Date', 'Location', 'Country'   # Event Metadata (optional: you can keep if you plan to use later)
]

df = df.drop(columns=columns_to_drop)

print(f"Dropped {len(columns_to_drop)} non-informative columns.")

# Step 3.6.2: Double-check remaining columns
df.columns.tolist()

# Distribution of Target Variable (Winner)
plt.figure(figsize=(6, 4))
sns.countplot(x='Winner', data=df, palette='Set2')
plt.title('Distribution of Fight Winners')
plt.xlabel('Winner')
plt.ylabel('Count')
plt.show()

# Distribution of 'Finish' (Finished vs Decision)
plt.figure(figsize=(6, 4))
sns.countplot(x='Finish', data=df, palette='Set3')
plt.title('Distribution of Fight Finish Type (Finished vs Decision)')
plt.xlabel('Finish Type')
plt.ylabel('Count')
plt.show()

"""#It appears that most of the fights end by Unanimous descison, second most common result is ko/tko and third is submission

#It appears that red wins more than blue
"""

# Visualizing distributions of fighter attributes
fig, axes = plt.subplots(2, 3, figsize=(15, 10))

# Age distribution for Red and Blue fighters
sns.histplot(df['RedAge'], bins=20, kde=True, ax=axes[0, 0], color='red', label='Red Fighter')
sns.histplot(df['BlueAge'], bins=20, kde=True, ax=axes[0, 0], color='blue', label='Blue Fighter')
axes[0, 0].set_title('Age Distribution of Fighters')
axes[0, 0].legend()

# Height distribution for Red and Blue fighters
sns.histplot(df['RedHeightCms'], bins=20, kde=True, ax=axes[0, 1], color='red', label='Red Fighter')
sns.histplot(df['BlueHeightCms'], bins=20, kde=True, ax=axes[0, 1], color='blue', label='Blue Fighter')
axes[0, 1].set_title('Height Distribution of Fighters')
axes[0, 1].legend()

# Reach distribution for Red and Blue fighters
sns.histplot(df['RedReachCms'], bins=20, kde=True, ax=axes[0, 2], color='red', label='Red Fighter')
sns.histplot(df['BlueReachCms'], bins=20, kde=True, ax=axes[0, 2], color='blue', label='Blue Fighter')
axes[0, 2].set_title('Reach Distribution of Fighters')
axes[0, 2].legend()

# Weight distribution for Red and Blue fighters
sns.histplot(df['RedWeightLbs'], bins=20, kde=True, ax=axes[1, 0], color='red', label='Red Fighter')
sns.histplot(df['BlueWeightLbs'], bins=20, kde=True, ax=axes[1, 0], color='blue', label='Blue Fighter')
axes[1, 0].set_title('Weight Distribution of Fighters')
axes[1, 0].legend()

# Significant Strikes Landed (Average for Red and Blue)
sns.histplot(df['RedAvgSigStrLanded'], bins=20, kde=True, ax=axes[1, 1], color='red', label='Red Fighter')
sns.histplot(df['BlueAvgSigStrLanded'], bins=20, kde=True, ax=axes[1, 1], color='blue', label='Blue Fighter')
axes[1, 1].set_title('Significant Strikes Landed')
axes[1, 1].legend()

# Takedowns Landed (Average for Red and Blue)
sns.histplot(df['RedAvgTDLanded'], bins=20, kde=True, ax=axes[1, 2], color='red', label='Red Fighter')
sns.histplot(df['BlueAvgTDLanded'], bins=20, kde=True, ax=axes[1, 2], color='blue', label='Blue Fighter')
axes[1, 2].set_title('Takedowns Landed')
axes[1, 2].legend()

plt.tight_layout()
plt.show()

# Boxplot to compare Red vs Blue Fighter Stats (Example: Age)
plt.figure(figsize=(8, 6))
sns.boxplot(x='Winner', y='RedAge', data=df, palette='Set1')
plt.title('Red Fighter Age vs Winner')
plt.xlabel('Winner')
plt.ylabel('Red Fighter Age')
plt.show()

# Scatterplot: Reach vs Winner (Does reach advantage increase chance of winning?)
plt.figure(figsize=(8, 6))
sns.scatterplot(x='RedReachCms', y='BlueReachCms', data=df, hue='Winner', palette='Set2', alpha=0.7)
plt.title('Reach Comparison: Red vs Blue Fighters')
plt.xlabel('Red Reach (cms)')
plt.ylabel('Blue Reach (cms)')
plt.show()

# Scatterplot: Takedowns vs Finish Type (Does Takedown average predict more finishes?)
plt.figure(figsize=(8, 6))
sns.scatterplot(x='RedAvgTDLanded', y='BlueAvgTDLanded', data=df, hue='Finish', palette='coolwarm', alpha=0.7)
plt.title('Takedowns vs Finish Type')
plt.xlabel('Red Takedowns')
plt.ylabel('Blue Takedowns')
plt.show()

# Age vs Winning Probability (For Red Fighter)
plt.figure(figsize=(8, 6))
sns.boxplot(x='Winner', y='RedAge', data=df, palette='Set2')
plt.title('Age vs Winner (Red Fighter)')
plt.xlabel('Winner')
plt.ylabel('Red Age')
plt.show()

# Significant Strikes Landed vs Finish Outcome (Red Fighter)
plt.figure(figsize=(8, 6))
sns.boxplot(x='Finish', y='RedAvgSigStrLanded', data=df, palette='Set1')
plt.title('Finish vs Significant Strikes Landed (Red Fighter)')
plt.xlabel('Finish Type')
plt.ylabel('Red Fighter Significant Strikes Landed')
plt.show()

# Number of Rounds vs Finish (More rounds â†’ More decisions?)
plt.figure(figsize=(8, 6))
sns.boxplot(x='NumberOfRounds', y='Finish', data=df, palette='pastel')
plt.title('Number of Rounds vs Finish Type')
plt.xlabel('Number of Rounds')
plt.ylabel('Finish Type')
plt.show()

# Finish Type by Weight Class
plt.figure(figsize=(16, 12))
sns.countplot(x='WeightClass', hue='Finish', data=df, palette='Set3')
plt.title('Finish Type by Weight Class')
plt.xlabel('Weight Class')
plt.ylabel('Count')
plt.show()

# How often are fights finished early? (1st/2nd/3rd Round)
plt.figure(figsize=(8, 6))
sns.countplot(x='FinishRound', hue='Finish', data=df[df['FinishRound'] <= 3], palette='coolwarm')
plt.title('Finish Frequency in Early Rounds')
plt.xlabel('Round')
plt.ylabel('Count')
plt.show()

"""#In heavier weight classes, tko/kos are more common.

"""

# Convert 'FinishRoundTime' from 'minutes:seconds' format to total seconds
def convert_to_seconds(time_str):
    try:
        minutes, seconds = map(int, time_str.split(':'))
        return minutes * 60 + seconds
    except:
        return 0  # Handle cases where the conversion fails (e.g., missing or malformed data)

df['FinishRoundTimeSecs'] = df['FinishRoundTime'].apply(convert_to_seconds)

# If 'TotalFightTimeSecs' is a string, ensure it is numeric
df['TotalFightTimeSecs'] = pd.to_numeric(df['TotalFightTimeSecs'], errors='coerce')

# Now, use the updated columns in the correlation matrix
correlation_columns = ['RedAge', 'BlueAge', 'RedHeightCms', 'BlueHeightCms', 'RedReachCms', 'BlueReachCms',
                       'RedWeightLbs', 'BlueWeightLbs', 'RedAvgSigStrLanded', 'BlueAvgSigStrLanded',
                       'RedAvgTDLanded', 'BlueAvgTDLanded', 'NumberOfRounds', 'RedCurrentWinStreak',
                       'BlueCurrentWinStreak', 'RedWins', 'BlueWins', 'FinishRoundTimeSecs', 'TotalFightTimeSecs']
correlation_matrix = df[correlation_columns].corr()

# Plot the correlation heatmap again
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Heatmap of Fighter Stats')
plt.show()

# Encode 'Winner' as numeric (Red = 1, Blue = 0)
df['WinnerNumeric'] = df['Winner'].apply(lambda x: 1 if x == 'Red' else 0)

# Updated correlation columns with stat differences (Dif columns) and numeric 'Winner'
correlation_columns_dif = ['SigStrDif', 'HeightDif', 'ReachDif', 'AgeDif', 'AvgSubAttDif',
                           'AvgTDDif', 'WinStreakDif', 'LoseStreakDif', 'LongestWinStreakDif',
                           'KODif', 'SubDif', 'TotalRoundDif', 'TotalTitleBoutDif']

# Adding the important columns like 'Finish' or the encoded 'Winner' for correlation analysis
correlation_columns_dif += ['WinnerNumeric', 'FinishRoundTimeSecs', 'TotalFightTimeSecs']

# Calculate correlation matrix for the difference columns
correlation_matrix_dif = df[correlation_columns_dif].corr()

# Plot the correlation heatmap again
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix_dif, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Heatmap: Stat Differences vs Outcome')
plt.show()

# Updated encoding for 'Finish' column with the 'Overturned' class included
finish_mapping = {
    'KO/TKO': 0,        # Knockout/TKO finish
    'SUB': 1,           # Submission finish
    'U-DEC': 2,         # Unanimous Decision
    'S-DEC': 2,         # Split Decision
    'M-DEC': 2,         # Majority Decision
    'DQ': 3,            # Disqualification
    'Unknown': 4,       # Unknown finish type
    'Overturned': 5     # Overturned fights
}

# Apply the mapping to the 'Finish' column
df['FinishNumeric'] = df['Finish'].map(finish_mapping)

# Now you can include 'FinishNumeric' in your correlation analysis
correlation_columns_dif = ['SigStrDif', 'HeightDif', 'ReachDif', 'AgeDif', 'AvgSubAttDif',
                           'AvgTDDif', 'WinStreakDif', 'LoseStreakDif', 'LongestWinStreakDif',
                           'KODif', 'SubDif', 'TotalRoundDif', 'TotalTitleBoutDif']

# Adding encoded 'WinnerNumeric' and 'FinishNumeric' for correlation analysis
correlation_columns_dif += ['WinnerNumeric', 'FinishNumeric', 'FinishRoundTimeSecs', 'TotalFightTimeSecs']

# Calculate correlation matrix for the difference columns
correlation_matrix_dif = df[correlation_columns_dif].corr()

# Plot the correlation heatmap again
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix_dif, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Heatmap: Stat Differences vs Outcome')
plt.show()

"""#Fighters getting more takedowns,get more submissions"""

df['Finish'].unique()

# Frequency of finishes by weight class
plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='WeightClass', hue='FinishNumeric')
plt.title('Finish Type Distribution by Weight Class')
plt.xlabel('Weight Class')
plt.ylabel('Frequency')
plt.xticks(rotation=90)
plt.show()

# Frequency of winners by weight class
plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='WeightClass', hue='WinnerNumeric')
plt.title('Winner Distribution by Weight Class')
plt.xlabel('Weight Class')
plt.ylabel('Frequency')
plt.xticks(rotation=90)
plt.show()

# Win streak vs outcome
plt.figure(figsize=(12, 6))
sns.boxplot(data=df, x='WinnerNumeric', y='WinStreakDif')
plt.title('Win Streak Difference vs Outcome')
plt.xlabel('Winner (0: Red, 1: Blue)')
plt.ylabel('Win Streak Difference')
plt.show()

# Loss streak vs outcome
plt.figure(figsize=(12, 6))
sns.boxplot(data=df, x='WinnerNumeric', y='LoseStreakDif')
plt.title('Loss Streak Difference vs Outcome')
plt.xlabel('Winner (0: Red, 1: Blue)')
plt.ylabel('Loss Streak Difference')
plt.show()

# Age difference vs outcome
plt.figure(figsize=(12, 6))
sns.boxplot(data=df, x='WinnerNumeric', y='AgeDif')
plt.title('Age Difference vs Outcome')
plt.xlabel('Winner (0: Red, 1: Blue)')
plt.ylabel('Age Difference')
plt.show()

# Total fight time vs finish type
plt.figure(figsize=(12, 6))
sns.boxplot(data=df, x='FinishNumeric', y='TotalFightTimeSecs')
plt.title('Total Fight Time vs Finish Type')
plt.xlabel('Finish Type')
plt.ylabel('Total Fight Time (Seconds)')
plt.show()

"""#Early kos are quite common. Decision can only be given at the end of a fight

"""

# Significant strikes vs finish type
plt.figure(figsize=(12, 6))
sns.scatterplot(data=df, x='RedAvgSigStrLanded', y='BlueAvgSigStrLanded', hue='FinishNumeric')
plt.title('Significant Strikes vs Finish Type')
plt.xlabel('Red Fighter Sig Str Landed')
plt.ylabel('Blue Fighter Sig Str Landed')
plt.show()

# Takedowns vs finish type
plt.figure(figsize=(12, 6))
sns.scatterplot(data=df, x='RedAvgTDLanded', y='BlueAvgTDLanded', hue='FinishNumeric')
plt.title('Takedowns vs Finish Type')
plt.xlabel('Red Fighter Takedowns Landed')
plt.ylabel('Blue Fighter Takedowns Landed')
plt.show()

# Title bout vs finish type
plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='TitleBout', hue='FinishNumeric')
plt.title('Finish Type by Title Bout')
plt.xlabel('Title Bout (0: No, 1: Yes)')
plt.ylabel('Frequency')
plt.show()

# Title bout vs outcome
plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='TitleBout', hue='WinnerNumeric')
plt.title('Outcome by Title Bout')
plt.xlabel('Title Bout (0: No, 1: Yes)')
plt.ylabel('Frequency')
plt.show()

# Stance vs outcome
plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='RedStance', hue='WinnerNumeric')
plt.title('Red Fighter Stance vs Outcome')
plt.xlabel('Red Fighter Stance')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='BlueStance', hue='WinnerNumeric')
plt.title('Blue Fighter Stance vs Outcome')
plt.xlabel('Blue Fighter Stance')
plt.ylabel('Frequency')
plt.show()

# Finish by round
plt.figure(figsize=(12, 6))
sns.countplot(data=df, x='FinishRound', hue='FinishNumeric')
plt.title('Finish Type by Round')
plt.xlabel('Round of Finish')
plt.ylabel('Frequency')
plt.show()

# Comparison of rankings vs winner
plt.figure(figsize=(12, 6))
sns.scatterplot(data=df, x='RMatchWCRank', y='BMatchWCRank', hue='WinnerNumeric')
plt.title('Red vs Blue Fighter Ranking vs Outcome')
plt.xlabel('Red Fighter Rank')
plt.ylabel('Blue Fighter Rank')
plt.show()

"""#Higher ranked fighters generally win more fights

"""

# Historical rounds fought vs winner
plt.figure(figsize=(12, 6))
sns.boxplot(data=df, x='WinnerNumeric', y='RedTotalRoundsFought')
plt.title('Red Fighter Total Rounds Fought vs Outcome')
plt.xlabel('Winner (0: Red, 1: Blue)')
plt.ylabel('Red Fighter Total Rounds Fought')
plt.show()

plt.figure(figsize=(12, 6))
sns.boxplot(data=df, x='WinnerNumeric', y='BlueTotalRoundsFought')
plt.title('Blue Fighter Total Rounds Fought vs Outcome')
plt.xlabel('Winner (0: Red, 1: Blue)')
plt.ylabel('Blue Fighter Total Rounds Fought')
plt.show()

# Submission vs KO analysis
plt.figure(figsize=(12, 6))
sns.boxplot(data=df, x='FinishNumeric', y='RedAvgSubAtt')
plt.title('Red Fighter Submission Attempts vs Finish Type')
plt.xlabel('Finish Type')
plt.ylabel('Red Fighter Average Submission Attempts')
plt.show()

plt.figure(figsize=(12, 6))
sns.boxplot(data=df, x='FinishNumeric', y='RedAvgSigStrLanded')
plt.title('Red Fighter Significant Strikes Landed vs Finish Type')
plt.xlabel('Finish Type')
plt.ylabel('Red Fighter Average Significant Strikes Landed')
plt.show()

"""#more signifacant strikes lead to more kos and more takedowns lead to more submissions"""

# Select numerical columns
numerical_features = [
    'RedAge', 'BlueAge', 'RedWeightLbs', 'BlueWeightLbs', 'RedHeightCms', 'BlueHeightCms',
    'RedReachCms', 'BlueReachCms', 'RedAvgSigStrLanded', 'BlueAvgSigStrLanded',
    'RedAvgTDLanded', 'BlueAvgTDLanded', 'RedAvgSubAtt', 'BlueAvgSubAtt',
    'RedCurrentWinStreak', 'BlueCurrentWinStreak', 'WinStreakDif', 'LoseStreakDif',
    'RedLosses', 'BlueLosses', 'RedWins', 'BlueWins', 'RedTotalRoundsFought',
    'BlueTotalRoundsFought', 'RedTotalTitleBouts', 'BlueTotalTitleBouts',
    'RedOdds', 'BlueOdds', 'RedDecOdds', 'BlueDecOdds', 'RSubOdds', 'BSubOdds',
    'RKOOdds', 'BKOOdds', 'TotalFightTimeSecs', 'FinishRoundTimeSecs'
]

# Calculate correlation matrix for numerical features
corr_matrix = df[numerical_features].corr()

# Plot heatmap of correlation matrix
plt.figure(figsize=(28, 16))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix of Numerical Features')
plt.show()

from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Encode categorical columns (e.g., 'Winner', 'RedStance', 'BlueStance')
df['WinnerNumeric'] = label_encoder.fit_transform(df['Winner'])  # 'Red' -> 0, 'Blue' -> 1
df['RedStance'] = label_encoder.fit_transform(df['RedStance'])
df['BlueStance'] = label_encoder.fit_transform(df['BlueStance'])

# Similarly, encode other categorical columns if needed

# Check for columns with string data
categorical_columns = df.select_dtypes(include=['object']).columns
print(f"Categorical columns: {categorical_columns}")

df['FinishNumeric'].unique()

# Drop unnecessary string columns (e.g., non-useful columns like 'Winner' if not encoded properly)
df = df.drop(columns=[ 'Finish', 'FinishRoundTime'])

df.columns.tolist()

df.dtypes
df=df.drop(columns=['Winner', 'FinishDetails'])

# One-Hot Encoding for columns with multiple categories
df = pd.get_dummies(df, columns=['WeightClass', 'Gender'], drop_first=True)
df['BetterRank'] = label_encoder.fit_transform(df['BetterRank'])

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Prepare features (X) and target variables (y)
X = df.drop(columns=['WinnerNumeric', 'FinishNumeric'])  # Features (exclude target columns)
y_winner = df['WinnerNumeric']  # Target: Fight Winner
y_finish = df['FinishNumeric']  # Target: Fight Finish

# Step 2: Remove post-fight columns that are irrelevant for prediction
# post_fight_columns = ['FinishRound', 'FinishRoundTimeSecs', 'TotalFightTimeSecs']
# df = df.drop(columns=post_fight_columns)

# Step 3: Split data into training and testing sets for 'Winner' prediction
X_train, X_test, y_train, y_test = train_test_split(X, y_winner, test_size=0.2, random_state=42)

# Initialize and train the Random Forest model for 'Winner' prediction
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Get feature importance for 'Winner' prediction
feature_importance_winner = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

# Plot feature importance for 'Winner' prediction
plt.figure(figsize=(48,24))
sns.barplot(x='Importance', y='Feature', data=feature_importance_winner)
plt.title('Feature Importance for Predicting Winner')
plt.show()

# Step 4: Similarly, train for Finish prediction
X_train, X_test, y_train, y_test = train_test_split(X, y_finish, test_size=0.2, random_state=42)
rf_model.fit(X_train, y_train)

# Get feature importance for 'Finish' prediction
feature_importance_finish = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

# Plot feature importance for 'Finish' prediction
plt.figure(figsize=(48,24))
sns.barplot(x='Importance', y='Feature', data=feature_importance_finish)
plt.title('Feature Importance for Predicting Finish')
plt.show()

from sklearn.metrics import accuracy_score, classification_report

# Step 1: Prepare the feature set and target variables for both Winner and Finish prediction

# For Winner Prediction
X_train_winner, X_test_winner, y_train_winner, y_test_winner = train_test_split(X, y_winner, test_size=0.2, random_state=42)

# Initialize and train RandomForest for Winner
rf_model_winner = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model_winner.fit(X_train_winner, y_train_winner)

# Predict the winner for test data
y_pred_winner = rf_model_winner.predict(X_test_winner)

# Step 2: Evaluate Winner model
print("Winner Prediction Metrics:")
print("Accuracy:", accuracy_score(y_test_winner, y_pred_winner))
print(classification_report(y_test_winner, y_pred_winner))

# For Finish Prediction
X_train_finish, X_test_finish, y_train_finish, y_test_finish = train_test_split(X, y_finish, test_size=0.2, random_state=42)

# Initialize and train RandomForest for Finish
rf_model_finish = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model_finish.fit(X_train_finish, y_train_finish)

# Predict the finish for test data
y_pred_finish = rf_model_finish.predict(X_test_finish)

# Step 3: Evaluate Finish model
print("Finish Prediction Metrics:")
print("Accuracy:", accuracy_score(y_test_finish, y_pred_finish))
print(classification_report(y_test_finish, y_pred_finish))

from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score

# For AUC-ROC
print("AUC-ROC:", roc_auc_score(y_test_winner, y_pred_winner))

# For confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test_winner, y_pred_winner))

# For F1-score
print("F1-Score:", f1_score(y_test_winner, y_pred_winner, average='weighted'))

from sklearn.metrics import roc_auc_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import label_binarize

# AUC-ROC for Multi-class Classification
y_test_finish_bin = label_binarize(y_test_finish, classes=[0, 1, 2, 3, 4, 5])  # Binarizing the target
y_pred_finish_prob = rf_model_finish.predict_proba(X_test_finish)  # Get predicted probabilities

# Calculate AUC-ROC
roc_auc_finish = roc_auc_score(y_test_finish_bin, y_pred_finish_prob, average='macro', multi_class='ovr')
print(f"AUC-ROC Score for Finish Model: {roc_auc_finish}")

# Confusion Matrix
cm_finish = confusion_matrix(y_test_finish, y_pred_finish)

# Plot Confusion Matrix using ConfusionMatrixDisplay
disp_finish = ConfusionMatrixDisplay(confusion_matrix=cm_finish, display_labels=['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5'])
disp_finish.plot(cmap='Blues', values_format='d')
plt.title('Confusion Matrix for Finish Prediction (Random Forest)')
plt.show()

X_train_winner.columns.tolist()

X_train_finish.columns.tolist()

leakage_columns = [
    'FinishRound',
    'TotalFightTimeSecs',
    'FinishRoundTimeSecs'
]

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Step 0: Drop leakage columns
leakage_columns = [
    'FinishRound',
    'TotalFightTimeSecs',
    'FinishRoundTimeSecs'
]

# Step 1: Prepare features (X) and target variables (y)
X = df.drop(columns=['WinnerNumeric', 'FinishNumeric'] + leakage_columns)  # Features
y_winner = df['WinnerNumeric']  # Target for winner prediction

# Step 2: Split data
X_train, X_test, y_train, y_test = train_test_split(X, y_winner, test_size=0.2, random_state=42)

# Step 3: Train Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Step 4: Evaluate model
y_pred = rf_model.predict(X_test)

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))

# Classification report
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Confusion matrix
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Step 5: Feature importance plot
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(24,12))
sns.barplot(x='Importance', y='Feature', data=feature_importance)
plt.title('Feature Importance for Predicting Winner')
plt.show()

# Step 1: Prepare features (X) and target variable (y) for Finish prediction
X = df.drop(columns=['WinnerNumeric', 'FinishNumeric'] + leakage_columns)  # Same clean features
y_finish = df['FinishNumeric']  # Target for Finish prediction

# Step 2: Split data
X_train, X_test, y_train, y_test = train_test_split(X, y_finish, test_size=0.2, random_state=42)

# Step 3: Train Random Forest model
rf_model_finish = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model_finish.fit(X_train, y_train)

# Step 4: Evaluate model
y_pred_finish = rf_model_finish.predict(X_test)

# Accuracy
print("Finish Prediction - Accuracy:", accuracy_score(y_test, y_pred_finish))

# Classification report
print("\nFinish Prediction - Classification Report:\n", classification_report(y_test, y_pred_finish))

# Confusion matrix
print("\nFinish Prediction - Confusion Matrix:\n", confusion_matrix(y_test, y_pred_finish))

# Step 5: Feature importance plot for Finish prediction
feature_importance_finish = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model_finish.feature_importances_
}).sort_values(by='Importance', ascending=False)

plt.figure(figsize=(24,12))
sns.barplot(x='Importance', y='Feature', data=feature_importance_finish)
plt.title('Feature Importance for Predicting Finish')
plt.show()

# Merge FinishNumeric rare classes
df['FinishNumeric'] = df['FinishNumeric'].replace({3: 2, 4: 2, 5: 2})
# Prepare features and target
X = df.drop(columns=['WinnerNumeric', 'FinishNumeric'] + leakage_columns)
y_finish = df['FinishNumeric']

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y_finish, test_size=0.2, random_state=42)

# Train RF with class_weight
rf_model_finish = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf_model_finish.fit(X_train, y_train)

# Predict and evaluate
y_pred_finish = rf_model_finish.predict(X_test)

print("New Finish Prediction - Accuracy:", accuracy_score(y_test, y_pred_finish))
print("\nNew Finish Prediction - Classification Report:\n", classification_report(y_test, y_pred_finish))
print("\nNew Finish Prediction - Confusion Matrix:\n", confusion_matrix(y_test, y_pred_finish))

from sklearn.ensemble import RandomForestClassifier

# Use class_weight balanced
rf_model_finish = RandomForestClassifier(
    n_estimators=300,
    max_depth=10,
    class_weight='balanced',
    random_state=42
)
rf_model_finish.fit(X_train, y_train)

# Predict and evaluate
y_pred_finish = rf_model_finish.predict(X_test)

print("Balanced Finish Prediction - Accuracy:", accuracy_score(y_test, y_pred_finish))
print("\nBalanced Finish Prediction - Classification Report:\n", classification_report(y_test, y_pred_finish))
print("\nBalanced Finish Prediction - Confusion Matrix:\n", confusion_matrix(y_test, y_pred_finish))

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Build the model
xgb_model_finish = XGBClassifier(
    n_estimators=500,
    learning_rate=0.03,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    use_label_encoder=False,
    eval_metric='mlogloss'
)

# Train
xgb_model_finish.fit(X_train, y_train)

# Predict
y_pred_finish = xgb_model_finish.predict(X_test)

# Evaluate
print("XGBoost Finish Prediction - Accuracy:", accuracy_score(y_test, y_pred_finish))
print("\nXGBoost Finish Prediction - Classification Report:\n", classification_report(y_test, y_pred_finish))
print("\nXGBoost Finish Prediction - Confusion Matrix:\n", confusion_matrix(y_test, y_pred_finish))

# Step 1: Prepare the dataset for the winner model
# Remove rank columns for individual weight classes and 'WinnerNumeric', 'FinishNumeric' which have already been dropped
winner_model_columns = [col for col in df.columns if 'Rank' not in col]
df_winner = df[winner_model_columns].copy()  # Ensure we work with a copy, not a view

# Create a new column 'RankDifference' for the relative rank between the fighters
df_winner['RankDifference'] = df['RMatchWCRank'] - df['BMatchWCRank']

# Drop all the individual rank columns
df_winner = df_winner.drop(columns=[col for col in df_winner.columns if 'Rank' in col])

# Step 2: Prepare the dataset for the finish model
# Remove rank fields but keep the "WeightClass" column
finish_model_columns = [col for col in df.columns if 'Rank' not in col]
df_finish = df[finish_model_columns].copy()  # Ensure we work with a copy, not a view

# Now, both df_winner and df_finish are ready for training

# Optionally, you can also re-split the data into train and test sets for both models
from sklearn.model_selection import train_test_split

# Train-test split for Winner model
X_winner = df_winner.drop(columns=['WinnerNumeric', 'FinishNumeric'])  # If you already dropped these columns, no need to drop them here
y_winner = df_winner['WinnerNumeric']  # Keep 'WinnerNumeric' for target
X_train_winner, X_test_winner, y_train_winner, y_test_winner = train_test_split(X_winner, y_winner, test_size=0.2, random_state=42)

# Train-test split for Finish model
X_finish = df_finish.drop(columns=['WinnerNumeric', 'FinishNumeric'])  # If you already dropped these columns, no need to drop them here
y_finish = df_finish['FinishNumeric']  # Keep 'FinishNumeric' for target
X_train_finish, X_test_finish, y_train_finish, y_test_finish = train_test_split(X_finish, y_finish, test_size=0.2, random_state=42)

# Now both datasets are clean and ready to be used for model

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Define and train the model for Winner Prediction
xgb_winner = XGBClassifier(
    objective='multi:softmax',
    num_class=3,  # Assuming 3 classes (Red/Blue winner)
    eval_metric='mlogloss',
    random_state=42,
    use_label_encoder=False
)

# Train the model
xgb_winner.fit(X_train_winner, y_train_winner)

# Predict on the test set
y_pred_winner = xgb_winner.predict(X_test_winner)

# Evaluate the Winner Prediction model
print("Winner Prediction - Accuracy:", accuracy_score(y_test_winner, y_pred_winner))
print("\nWinner Prediction - Classification Report:\n", classification_report(y_test_winner, y_pred_winner))
print("\nWinner Prediction - Confusion Matrix:\n", confusion_matrix(y_test_winner, y_pred_winner))

# Define and train the model for Finish Prediction
xgb_finish = XGBClassifier(
    objective='multi:softmax',
    num_class=6,  # Assuming 6 possible finish types (KO, Submission, etc.)
    eval_metric='mlogloss',
    random_state=42,
    use_label_encoder=False
)

# Train the model
xgb_finish.fit(X_train_finish, y_train_finish)

# Predict on the test set
y_pred_finish = xgb_finish.predict(X_test_finish)

# Evaluate the Finish Prediction model
print("Finish Prediction - Accuracy:", accuracy_score(y_test_finish, y_pred_finish))
print("\nFinish Prediction - Classification Report:\n", classification_report(y_test_finish, y_pred_finish))
print("\nFinish Prediction - Confusion Matrix:\n", confusion_matrix(y_test_finish, y_pred_finish))

df_finish.columns.tolist()

df_winner.columns.tolist()

# Step 1: Extract relevant features for the Winner Prediction model
winner_features = [
    'RedOdds', 'BlueOdds', 'RedExpectedValue', 'BlueExpectedValue',
    'TitleBout', 'NumberOfRounds', 'RedCurrentLoseStreak', 'BlueCurrentLoseStreak',
    'RedCurrentWinStreak', 'BlueCurrentWinStreak', 'RedAvgSigStrLanded', 'BlueAvgSigStrLanded',
    'RedAvgSigStrPct', 'BlueAvgSigStrPct', 'RedAvgSubAtt', 'BlueAvgSubAtt',
    'RedAvgTDLanded', 'BlueAvgTDLanded', 'RedWinsByKO', 'BlueWinsByKO',
    'RedWinsBySubmission', 'BlueWinsBySubmission', 'RedWinsByDecisionUnanimous',
    'BlueWinsByDecisionUnanimous', 'RedWins', 'BlueWins', 'RedHeightCms',
    'BlueHeightCms', 'RedReachCms', 'BlueReachCms', 'RedWeightLbs', 'BlueWeightLbs',
    'WinStreakDif', 'LoseStreakDif', 'HeightDif', 'ReachDif', 'AgeDif', 'SigStrDif',
    'AvgSubAttDif', 'AvgTDDif'
]

# Step 2: Prepare the dataset for training
df_winner = df[winner_features + ['WinnerNumeric']]

# Step 3: Split the dataset into train and test sets
from sklearn.model_selection import train_test_split

X_winner = df_winner.drop(columns=['WinnerNumeric'])
y_winner = df_winner['WinnerNumeric']

X_train_winner, X_test_winner, y_train_winner, y_test_winner = train_test_split(X_winner, y_winner, test_size=0.2, random_state=42)

# Step 4: Train the model (XGBoost)
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

xgb = XGBClassifier(objective='multi:softmax', num_class=2, eval_metric='mlogloss', use_label_encoder=False)

# Train the model
xgb.fit(X_train_winner, y_train_winner)

# Step 5: Evaluate the model
y_pred_winner = xgb.predict(X_test_winner)

# Accuracy and Classification Report
print("Winner Prediction - Accuracy:", accuracy_score(y_test_winner, y_pred_winner))
print("\nWinner Prediction - Classification Report:\n", classification_report(y_test_winner, y_pred_winner))
print("\nWinner Prediction - Confusion Matrix:\n", confusion_matrix(y_test_winner, y_pred_winner))

# Step 1: Extract relevant features for the Finish Prediction model
finish_features = [
    'RedOdds', 'BlueOdds', 'RedExpectedValue', 'BlueExpectedValue',
    'TitleBout', 'NumberOfRounds', 'RedCurrentLoseStreak', 'BlueCurrentLoseStreak',
    'RedCurrentWinStreak', 'BlueCurrentWinStreak', 'RedAvgSigStrLanded', 'BlueAvgSigStrLanded',
    'RedAvgSigStrPct', 'BlueAvgSigStrPct', 'RedAvgSubAtt', 'BlueAvgSubAtt',
    'RedAvgTDLanded', 'BlueAvgTDLanded', 'RedWinsByKO', 'BlueWinsByKO',
    'RedWinsBySubmission', 'BlueWinsBySubmission', 'RedWinsByDecisionUnanimous',
    'BlueWinsByDecisionUnanimous', 'RedWins', 'BlueWins', 'RedHeightCms',
    'BlueHeightCms', 'RedReachCms', 'BlueReachCms', 'RedWeightLbs', 'BlueWeightLbs',
    'WinStreakDif', 'LoseStreakDif', 'HeightDif', 'ReachDif', 'AgeDif', 'SigStrDif',
    'AvgSubAttDif', 'AvgTDDif', 'RedDecOdds', 'BlueDecOdds', 'RSubOdds', 'BSubOdds',
    'RKOOdds', 'BKOOdds'
]

# Step 2: Prepare the dataset for training
df_finish = df[finish_features + ['FinishNumeric']]

# Step 3: Split the dataset into train and test sets
X_finish = df_finish.drop(columns=['FinishNumeric'])
y_finish = df_finish['FinishNumeric']

from sklearn.model_selection import train_test_split

X_train_finish, X_test_finish, y_train_finish, y_test_finish = train_test_split(X_finish, y_finish, test_size=0.2, random_state=42)

# Step 4: Train the model (XGBoost)
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

xgb_finish = XGBClassifier(objective='multi:softmax', num_class=3, eval_metric='mlogloss', use_label_encoder=False)

# Train the model
xgb_finish.fit(X_train_finish, y_train_finish)

# Step 5: Evaluate the model
y_pred_finish = xgb_finish.predict(X_test_finish)

# Accuracy and Classification Report
print("Finish Prediction - Accuracy:", accuracy_score(y_test_finish, y_pred_finish))
print("\nFinish Prediction - Classification Report:\n", classification_report(y_test_finish, y_pred_finish))
print("\nFinish Prediction - Confusion Matrix:\n", confusion_matrix(y_test_finish, y_pred_finish))

from imblearn.over_sampling import SMOTE, ADASYN
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Assuming X_train_winner and y_train_winner are already defined for the winner prediction model

# Apply SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_winner, y_train_winner)

# Train the model with SMOTE data
xgb = XGBClassifier(objective='multi:softmax', num_class=3, eval_metric='mlogloss', random_state=42)
xgb.fit(X_train_smote, y_train_smote)

# Make predictions
y_pred_smote = xgb.predict(X_test_winner)

# Evaluate the model
print("Winner Prediction (SMOTE) - Accuracy:", accuracy_score(y_test_winner, y_pred_smote))
print("\nWinner Prediction (SMOTE) - Classification Report:\n", classification_report(y_test_winner, y_pred_smote))
print("\nWinner Prediction (SMOTE) - Confusion Matrix:\n", confusion_matrix(y_test_winner, y_pred_smote))

# Apply ADASYN
adasyn = ADASYN(random_state=42)
X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_winner, y_train_winner)

# Train the model with ADASYN data
xgb.fit(X_train_adasyn, y_train_adasyn)

# Make predictions
y_pred_adasyn = xgb.predict(X_test_winner)

# Evaluate the model
print("\nWinner Prediction (ADASYN) - Accuracy:", accuracy_score(y_test_winner, y_pred_adasyn))
print("\nWinner Prediction (ADASYN) - Classification Report:\n", classification_report(y_test_winner, y_pred_adasyn))
print("\nWinner Prediction (ADASYN) - Confusion Matrix:\n", confusion_matrix(y_test_winner, y_pred_adasyn))

from imblearn.over_sampling import SMOTE, ADASYN
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_finish, y_train_finish)

# Train the XGBoost model with SMOTE data
model_smote = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
model_smote.fit(X_train_smote, y_train_smote)

# Evaluate the SMOTE model
y_pred_smote = model_smote.predict(X_test_finish)  # Ensure you have X_test_finish
print("Finish Prediction (SMOTE) - Accuracy:", accuracy_score(y_test_finish, y_pred_smote))
print("Finish Prediction (SMOTE) - Classification Report:\n", classification_report(y_test_finish, y_pred_smote))
print("Finish Prediction (SMOTE) - Confusion Matrix:\n", confusion_matrix(y_test_finish, y_pred_smote))

# ADASYN
adasyn = ADASYN(random_state=42)
X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_finish, y_train_finish)

# Train the XGBoost model with ADASYN data
model_adasyn = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
model_adasyn.fit(X_train_adasyn, y_train_adasyn)

# Evaluate the ADASYN model
y_pred_adasyn = model_adasyn.predict(X_test_finish)  # Ensure you have X_test_finish
print("Finish Prediction (ADASYN) - Accuracy:", accuracy_score(y_test_finish, y_pred_adasyn))
print("Finish Prediction (ADASYN) - Classification Report:\n", classification_report(y_test_finish, y_pred_adasyn))
print("Finish Prediction (ADASYN) - Confusion Matrix:\n", confusion_matrix(y_test_finish, y_pred_adasyn))

from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split

# Set the scale_pos_weight parameter
class_weights = {
    0: 1.5,  # Class 0 (finish type 0)
    1: 5,  # Class 1 (finish type 1) - increase weight to compensate for imbalance
    2: 1.5   # Class 2 (finish type 2)
}

# Train-test split if not already done
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the XGBoost model with class weights
model = XGBClassifier(scale_pos_weight=[class_weights[0], class_weights[1], class_weights[2]], use_label_encoder=False)
model.fit(X_train_finish, y_train_finish)

# Predict and evaluate the model
y_pred_finish = model.predict(X_test_finish)

# Evaluate the model
print("Finish Prediction with Class Weights - Accuracy:", accuracy_score(y_test_finish, y_pred_finish))
print("Finish Prediction with Class Weights - Classification Report:")
print(classification_report(y_test_finish, y_pred_finish))
print("Finish Prediction with Class Weights - Confusion Matrix:")
print(confusion_matrix(y_test_finish, y_pred_finish))

from imblearn.over_sampling import ADASYN
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score, StratifiedKFold
import numpy as np

# Initialize ADASYN with different n_neighbors or sampling strategy
adasyn = ADASYN(sampling_strategy='auto', n_neighbors=5, random_state=42)

# XGBoost model
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')

# Create pipeline
pipeline = Pipeline([('adasyn', adasyn), ('xgb', xgb)])

# Define cross-validation strategy
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Perform cross-validation
scores = cross_val_score(pipeline, X_train_finish, y_train_finish, cv=cv, scoring='accuracy')

# Print cross-validation results
print(f"Cross-validation accuracy scores: {scores}")
print(f"Mean accuracy: {np.mean(scores)}")
print(f"Standard deviation of accuracy: {np.std(scores)}")

# Train the model on the full training data and evaluate on the test set
pipeline.fit(X_train_finish, y_train_finish)

# Predict on the test set
y_pred = pipeline.predict(X_test_finish)

# Evaluate the final model on the test set
print("Finish Prediction (ADASYN) - Accuracy:", accuracy_score(y_test_finish, y_pred))
print("Finish Prediction (ADASYN) - Classification Report:\n", classification_report(y_test_finish, y_pred))
print("Finish Prediction (ADASYN) - Confusion Matrix:\n", confusion_matrix(y_test_finish, y_pred))

from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.pipeline import Pipeline

# SMOTE with adjusted k_neighbors
smote = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42)

# XGBoost model
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')

# Create pipeline
pipeline = Pipeline([('smote', smote), ('xgb', xgb)])

# Fit model
pipeline.fit(X_train, y_train)

# Predict
y_pred = pipeline.predict(X_test)

# Evaluate
print("Finish Prediction (SMOTE) - Accuracy:", accuracy_score(y_test, y_pred))
print("Finish Prediction (SMOTE) - Classification Report:\n", classification_report(y_test, y_pred))
print("Finish Prediction (SMOTE) - Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# Define models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'SVM': SVC(),
    'KNN': KNeighborsClassifier(),
    'Naive Bayes': GaussianNB(),
    'LightGBM': LGBMClassifier(),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
}

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_finish, y_finish, test_size=0.2, random_state=42)

# Initialize a dictionary to store results
results = {}

# Train and evaluate each model
for name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)

    # Predict on the test set
    y_pred = model.predict(X_test)

    # Evaluate accuracy and store results
    accuracy = model.score(X_test, y_test)
    report = classification_report(y_test, y_pred, output_dict=True)
    results[name] = {
        'Accuracy': accuracy,
        'Classification Report': report,
        'Confusion Matrix': confusion_matrix(y_test, y_pred)
    }

# Print the results for comparison
for name, result in results.items():
    print(f"Model: {name}")
    print(f"Accuracy: {result['Accuracy']}")
    print("Classification Report:")
    print(result['Classification Report'])
    print("Confusion Matrix:")
    print(result['Confusion Matrix'])
    print("\n" + "="*60 + "\n")

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_finish, y_finish, test_size=0.2, random_state=42)

# Initialize and fit the Logistic Regression model
log_reg = LogisticRegression(max_iter=1000, random_state=42)
log_reg.fit(X_train, y_train)

# Make predictions
y_pred = log_reg.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Regularization with C
log_reg = LogisticRegression(C=0.1, max_iter=1000)  # Smaller C means stronger regularization
log_reg.fit(X_train, y_train)

# Predictions and evaluation
y_pred = log_reg.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

from sklearn.model_selection import cross_val_score

# Perform 5-fold cross-validation
log_reg = LogisticRegression(max_iter=1000)
cv_scores = cross_val_score(log_reg, X_finish, y_finish, cv=5, scoring='accuracy')

# Average accuracy from cross-validation
print(f"Cross-validation scores: {cv_scores}")
print(f"Mean Accuracy: {cv_scores.mean()}")

from sklearn.preprocessing import StandardScaler

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_finish, test_size=0.2, random_state=42)

# Fit Logistic Regression
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)

# Predictions and evaluation
y_pred = log_reg.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

from sklearn.preprocessing import MinMaxScaler

# Normalize the features
scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_normalized, y_finish, test_size=0.2, random_state=42)

# Fit Logistic Regression
log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)

# Predictions and evaluation
y_pred = log_reg.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

from imblearn.over_sampling import ADASYN
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Assuming X_train_winner, y_train_winner, X_test_winner, y_test_winner are defined

# Apply ADASYN
adasyn = ADASYN(random_state=42)
X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_winner, y_train_winner)

# List of models to train
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "SVM": SVC(random_state=42),
    "KNN": KNeighborsClassifier(),
    "Naive Bayes": GaussianNB(),
    "XGBoost": XGBClassifier(objective='multi:softmax', num_class=3, eval_metric='mlogloss', random_state=42)
}

# Train, predict, and evaluate each model
for model_name, model in models.items():
    print(f"Training {model_name}...")

    # Train the model with ADASYN resampled data
    model.fit(X_train_adasyn, y_train_adasyn)

    # Make predictions
    y_pred = model.predict(X_test_winner)

    # Evaluate the model
    accuracy = accuracy_score(y_test_winner, y_pred)
    print(f"{model_name} Accuracy: {accuracy}")
    print(f"Classification Report:\n{classification_report(y_test_winner, y_pred)}")
    print(f"Confusion Matrix:\n{confusion_matrix(y_test_winner, y_pred)}")
    print("="*60)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Assuming X_train_winner and y_train_winner are your training data for the winner prediction model

# Step 1: Ensure that only numerical columns are selected
X_train_numeric = X_train_winner.select_dtypes(include=[np.number])

# Step 2: Calculate Q1 (25th percentile) and Q3 (75th percentile)
Q1 = X_train_numeric.quantile(0.25)
Q3 = X_train_numeric.quantile(0.75)
IQR = Q3 - Q1

# Step 3: Detect outliers
outliers = ((X_train_numeric < (Q1 - 1.5 * IQR)) | (X_train_numeric > (Q3 + 1.5 * IQR)))

# Step 4: Remove outliers from the data
X_train_no_outliers = X_train_numeric[~outliers.any(axis=1)]
y_train_no_outliers = y_train_winner[~outliers.any(axis=1)]

# Step 5: Standardize the data (scaling)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_no_outliers)

# Step 6: Train the SVM model
svm = SVC(kernel='linear', random_state=42)
svm.fit(X_train_scaled, y_train_no_outliers)

# Step 7: Make predictions
X_test_numeric = X_test_winner.select_dtypes(include=[np.number])
y_pred_no_outliers = svm.predict(scaler.transform(X_test_numeric))  # Scale the test data

# Step 8: Evaluate the model
print("SVM Model (No Outliers) - Accuracy:", accuracy_score(y_test_winner, y_pred_no_outliers))
print("\nSVM Model (No Outliers) - Classification Report:\n", classification_report(y_test_winner, y_pred_no_outliers))
print("\nSVM Model (No Outliers) - Confusion Matrix:\n", confusion_matrix(y_test_winner, y_pred_no_outliers))

from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Step 1: Get decision function scores
y_scores = svm.decision_function(scaler.transform(X_test_numeric))

# Step 2: Calculate AUC-ROC
roc_auc = roc_auc_score(y_test_winner, y_scores)
print(f"AUC-ROC Score: {roc_auc:.4f}")

# Step 3: Plot ROC Curve
fpr, tpr, thresholds = roc_curve(y_test_winner, y_scores)
plt.figure(figsize=(8,6))
plt.plot(fpr, tpr, color='blue', label=f'AUC = {roc_auc:.2f}')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Random guessing line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve (SVM, No Outliers)')
plt.legend(loc='lower right')
plt.grid()
plt.show()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, label_binarize
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve,auc
import matplotlib.pyplot as plt

# Assume your preprocessed data is: X and y_finish
# For demonstration, make sure you have X and y_finish as input data

# 1. Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 2. Split the data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_finish, test_size=0.2, random_state=42)

# 3. Fit Logistic Regression
log_reg = LogisticRegression(max_iter=1000, random_state=42)
log_reg.fit(X_train, y_train)

# 4. Predictions and Evaluation
y_pred = log_reg.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Logistic Regression Finish Prediction - Accuracy: {accuracy:.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

# 5. AUC-ROC Score (for multiclass)
y_proba = log_reg.predict_proba(X_test)  # Probabilities for all classes
auc_score = roc_auc_score(y_test, y_proba, multi_class='ovr')  # Multi-class One-vs-Rest strategy
print(f"\nAUC-ROC Score (Multiclass OVR): {auc_score:.4f}")

# 6. Plot AUC-ROC Curve
# Step 1: Binarize the labels for ROC curve
classes = np.unique(y_test)  # Assuming classes are [0, 1, 2]
y_test_bin = label_binarize(y_test, classes=classes)

# Step 2: Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(len(classes)):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_proba[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Step 3: Plot all ROC curves
plt.figure(figsize=(10, 8))

colors = ['blue', 'green', 'red']
for i, color in zip(range(len(classes)), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label=f'ROC curve for class {classes[i]} (area = {roc_auc[i]:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)  # Random guess line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=14)
plt.ylabel('True Positive Rate', fontsize=14)
plt.title('Multiclass ROC Curve (Finish Prediction)', fontsize=16)
plt.legend(loc="lower right")
plt.grid()
plt.show()